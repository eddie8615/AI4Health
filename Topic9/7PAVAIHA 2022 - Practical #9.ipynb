{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicle 9: Explainable AI with LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aim** Learn how to generate model explaininations using the *Local Interpretable Model-agnostic Explanations* (__[LIME](https://github.com/marcotcr/lime)__) toolbox. \n",
    "\n",
    "We be using the *Cleveland Heart Disease Data Set*, avialiable (__[here](https://archive.ics.uci.edu/ml/datasets/Heart+Disease)__) through the University of California Irvine Machine Learning Repository. A CSV of the data has been posted in Keats. Please download it before the lab and place it in the same folder you are running this program from.\n",
    "\n",
    "The dataset contains 14 clinical attributes from 303 indiviuduals. Each sample is a labelled with either presence (value 1) or absence (value 0)of heart disease. The features in the database are:\n",
    "1. age: age in years \n",
    "2. sex: 1 = male; 0 = female\n",
    "3. cp: chest pain type. 1= typical angina; 2 = atypical angina; 3 = non-anginal pain ; 4 = asymptomatic \n",
    "4. trestbps:resting blood pressure (in mm Hg on admission to the hospital) \n",
    "5. chol: serum cholestoral in mg/dl \n",
    "6. fbs:  fasting blood sugar > 120 mg/dl. 1 = true; 0 = false\n",
    "7. restecg: resting electrocardiographic results. 0 = normal; 1: having ST-T wave abnormality; 2 = probable or definite left ventricular hypertrophy\n",
    "8. thalach: maximum heart rate achieved\n",
    "9. exang: exercise induced angina. 1 = yes; 0 = no\n",
    "10. oldpeak: ST segment depression induced by exercise relative to rest \n",
    "11. slope: the slope of the peak exercise ST segment. 1 = upsloping; 2 = flat; 3 = downsloping \n",
    "12. ca: number of major vessels (0-3) colored by flourosopy \n",
    "13. thal: Thalassemia. 3 = normal; 6 = fixed defect; 7 = reversable defect \n",
    "14. target: Diagnosis of heart disease. Value 0 = < 50% diameter narrowing; Value 1 = 50% diameter narrowing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 Set-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to install LIME *before* starting the tutorial: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install lime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all toolboxes need to support the tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries necessary for this practical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import InputLayer\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.experimental import preprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import lime\n",
    "from lime import lime_tabular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observing Model training** the following function will be used to obsevere the behaviour of the models after they have been trained. \n",
    "It plots the loss and accuracy of model after every training epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learningCurve(history, epoch):\n",
    "  # Plot training & validation accuracy values\n",
    "  epoch_range = range(1, epoch+1)\n",
    "  plt.plot(epoch_range, history.history['accuracy'])\n",
    "  plt.plot(epoch_range, history.history['val_accuracy'])\n",
    "  plt.title('Model accuracy')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Val'], loc='upper left')\n",
    "  plt.show()\n",
    "\n",
    "  # Plot training & validation loss values\n",
    "  plt.plot(epoch_range, history.history['loss'])\n",
    "  plt.plot(epoch_range, history.history['val_loss'])\n",
    "  plt.title('Model loss')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Val'], loc='upper left')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 Load data and visualise key characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"heart.csv\")\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the distubution of the target scores and gender in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "sns.countplot(data=data, x=data.target.replace([1, 0], ['Has Disease', 'Healthy']), ax=ax1)\n",
    "sns.countplot(data=data, x=data.sex.replace([0, 1], ['Female', 'Male']), ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the distribution of disease by age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(data.age,data.target).plot(kind=\"bar\",figsize=(16, 7))\n",
    "plt.title('Heart Disease Frequency grouped by age')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Disease Frequency')\n",
    "plt.legend([\"Have Disease\", \"Healthy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's plot the distribution of disease by gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(data.sex.replace([0, 1], ['Female', 'Male']),data.target).plot(kind=\"bar\",figsize=(10, 5))\n",
    "plt.xticks(rotation=0)\n",
    "plt.title('Heart Disease Frequency grouped by Gender')\n",
    "plt.xlabel('Sex')\n",
    "plt.ylabel('Desiase Frequency')\n",
    "plt.legend([\"Have Disease\", \"Healthy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, plot the distribution of the features using the sns pairplot function\n",
    "- **Note** This code takes a couple of minutes to run\n",
    "\n",
    "Using these plots note down some initial hypothesis on what maybe important features in our classifier? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data, hue='target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 Perform the necessary preprocessing on data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert data from to required train and test arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate features from labes\n",
    "X = data.drop(columns='target')\n",
    "y = data['target']\n",
    "\n",
    "# Create train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42, stratify = y)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_train = X_train.values \n",
    "X_test = X_test.values \n",
    "\n",
    "y_train = y_train.values \n",
    "y_test = y_test.values\n",
    "\n",
    "# Parameters needed for setting the input shapes in our model\n",
    "n_features =  X_train.shape[1]\n",
    "\n",
    "# one hot encode labels for softmax\n",
    "y_train_vec = to_categorical(y_train, 2)\n",
    "y_test_vec = to_categorical(y_test, 2)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 Train Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Descibe the model being built below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=n_features, activation='relu'))  \n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** What are the key characteristic of our model training set-up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "\n",
    "model.compile(optimizer='adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "EPOCHS = 64\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train_vec, \n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split = 0.2,\n",
    "    shuffle=True,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "plot_learningCurve(history, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** We are using softmax and categorical_crossentropy in this task as LIME works with probabilistic outputs. If we did not need probabilistic outputs, what other final layer activation function and loss function combination could we use for this task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(X_test, y_test_vec)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "y_pred_vec = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_vec, axis=1)\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cf_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help find interesting example to expalin, we need identify all correctly and incorrectly classified examples per class. The following function identifes all True Positive, False Negative, False Positive and True Negative samples in the test data\n",
    "\n",
    "**Question** What do these terms (True Positive etc) mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_measure(y_actual, y_pred):\n",
    "    TP = []\n",
    "    FP = []\n",
    "    TN = []\n",
    "    FN = []\n",
    "    counter = 0\n",
    "\n",
    "    for i in range(len(y_pred)): \n",
    "        if y_actual[i]==y_pred[i]==1:\n",
    "           TP.append(counter)\n",
    "        if y_pred[i]==1 and y_actual[i]!=y_pred[i]:\n",
    "           FP.append(counter)\n",
    "        if y_actual[i]==y_pred[i]==0:\n",
    "           TN.append(counter)\n",
    "        if y_pred[i]==0 and y_actual[i]!=y_pred[i]:\n",
    "           FN.append(counter)\n",
    "        counter += 1\n",
    "\n",
    "    return(TP, FP, TN, FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "True_Positive, False_Negative, False_Positive, True_Negative = perf_measure(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 Create Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start explaining the model we need to create a tabular explainer `object`, which expects the following parameters:\n",
    "- `training_data`: The data used to train the model. This must be in a Numpy array format.\n",
    "- `feature_names`: The column names from the training set\n",
    "- `class_names`: The distinct classes from the target variable\n",
    "- `mode`: The type of problem being solved (classification in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime_tabular.LimeTabularExplainer(\n",
    "    training_data=X_train,\n",
    "    feature_names=X.columns,\n",
    "    class_names=['absence','presence'],\n",
    "    mode='classification'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can now start explaining!** \n",
    "\n",
    "Call the explain_instance function to explain the prediction. \n",
    "\n",
    "The following parameters are required:\n",
    "- `data_row`: a single observation from the dataset\n",
    "- `predict_fn`: a function used to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explaining a correctly classified heart disease instance. \n",
    "\n",
    "**Question** Do any of the features used in the explination match your inital hypothises on important features? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(\n",
    "    data_row=X_test[True_Positive[0]], \n",
    "    predict_fn=model.predict\n",
    ")\n",
    "\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explaining a incorrectly classified heart disease instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(\n",
    "    data_row=X_test[False_Positive[0]], \n",
    "    predict_fn=model.predict\n",
    ")\n",
    "\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explaining a correctly classified absence of heart disease instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(\n",
    "    data_row=X_test[True_Negative[0]], \n",
    "    predict_fn=model.predict\n",
    ")\n",
    "\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explaining a incorrectly classified absence of heart disease instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(\n",
    "    data_row=X_test[False_Negative[0]], \n",
    "    predict_fn=model.predict\n",
    ")\n",
    "\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create LIME explinations for a *different* classifier (prefereably not a slightly different feedforward neural network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement LIME explinations for the *Breast Cancer Wisconsin Diagnostic Dataset* from Practical 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "335px",
    "left": "1748px",
    "right": "20px",
    "top": "256px",
    "width": "567px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
