{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 8: CNNs and RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this lab is the learn to implement Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNN) in Keras.\n",
    "\n",
    "We will do this be implementing building CNN and RNN models for the task of *Human Activity Recognition* (HAR). This is the problem of predicting what a person is doing based on a trace of their movement using accelerometers sensors.\n",
    "\n",
    "The data, and a description of the task we are doing can be found __[here](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones)__.\n",
    "\n",
    "If you have not already done so, please download a copy of the daa form the __[UCI website](https://archive.ics.uci.edu/ml/machine-learning-databases/00240/)__ before the lab (the *UCI HAR Dataset.zip* folder). After downloading, unzip it and place it in the same folder you are running this program from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 Set-Up\n",
    "Import all toolboxes need to support the tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 15:01:33.212031: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/changhyun/catkin_ws/devel/lib:/opt/ros/noetic/lib\n",
      "2022-05-18 15:01:33.212057: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import InputLayer\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import GRU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import preprocessing\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observing Model training** the following function will be used to obsevere the behaviour of the models after they have been trained. It plots the loss and accuracy of model after every training epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learningCurve(history, epoch):\n",
    "  # Plot training & validation accuracy values\n",
    "  epoch_range = range(1, epoch+1)\n",
    "  plt.plot(epoch_range, history.history['accuracy'])\n",
    "  plt.plot(epoch_range, history.history['val_accuracy'])\n",
    "  plt.title('Model accuracy')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Val'], loc='upper left')\n",
    "  plt.show()\n",
    "\n",
    "  # Plot training & validation loss values\n",
    "  plt.plot(epoch_range, history.history['loss'])\n",
    "  plt.plot(epoch_range, history.history['val_loss'])\n",
    "  plt.title('Model loss')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Val'], loc='upper left')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 Load the data, partition and set up pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was collected from 30 subjects aged between 19 and 48 years old performing one of six standard activities while wearing a waist-mounted smartphone that recorded the movement data. The activities performed were:\n",
    "- Walking\n",
    "- Walking Upstairs\n",
    "- Walking Downstairs\n",
    "- Sitting\n",
    "- Standing\n",
    "- Laying\n",
    "\n",
    "There are three main signal types in the raw data: total acceleration, body acceleration, and body gyroscope. Each has 3 axises (x,y,z) of data. This means that there are a total of nine variables for each time step.\n",
    "\n",
    "Further, each series of data has been partitioned into overlapping windows of 2.56 seconds of data, or 128 time steps. These windows of data correspond to the windows of engineered features (rows) in the previous section.\n",
    "\n",
    "This means that one row of data has (128 * 9), or 1,152 elements\n",
    "\n",
    "The following block of code are the functions needed to exract this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "    dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
    "    return dataframe.values\n",
    " \n",
    "# load a list of files and return as a 3d numpy array\n",
    "def load_group(filenames, prefix='UCIHARDataset/'):\n",
    "    loaded = list()\n",
    "    for name in filenames:\n",
    "        data = load_file(prefix + name)\n",
    "        loaded.append(data)\n",
    "    # stack group so that features are the 3rd dimension\n",
    "    loaded = dstack(loaded)\n",
    "    return loaded\n",
    "\n",
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group, prefix='UCIHARDataset/'):\n",
    "    filepath = prefix + group + '/Inertial Signals/'\n",
    "    # load all 9 files as a single array\n",
    "    filenames = list()\n",
    "    # total acceleration\n",
    "    filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
    "    # body acceleration\n",
    "    filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
    "    # body gyroscope\n",
    "    filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
    "    # load input data\n",
    "    X = load_group(filenames, filepath)\n",
    "    # load class output\n",
    "    y = load_file(prefix + group + '/y_'+group+'.txt')\n",
    "    return X, y\n",
    "\n",
    " \n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(prefix='UCIHARDataset/'):\n",
    "    # load all train\n",
    "    trainX, trainy = load_dataset_group('train')\n",
    "    # load all test\n",
    "    testX, testy = load_dataset_group('test')\n",
    "    # zero-offset class values\n",
    "    trainy = trainy - 1\n",
    "    testy = testy - 1\n",
    "    # one hot encode y\n",
    "    trainy = to_categorical(trainy)\n",
    "    testy = to_categorical(testy)\n",
    "    return trainX, trainy, testX, testy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the `load_dataset` function, then split the training data into a training and a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract defined training and test splits\n",
    "X_train, y_train, X_test, y_test = load_dataset()\n",
    "\n",
    "# Parameters needed for setting the input and output shapes in our models\n",
    "n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block of code utiliser the Keras preprocessing layers API to build Keras-native input pre-processing step. We implemetn the normiliser in the a `create_model()` function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = preprocessing.Normalization()\n",
    "normalizer.adapt(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 4 Building a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous lab, we build our CNN via a keras __[sequential](https://keras.io/guides/sequential_model/)__ model. \n",
    "\n",
    "**Questions:** \n",
    "- Describe the architecture of the model coded below.\n",
    "- What is the purpose of the `BatchNormalization()` layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(n_timesteps,n_features)))\n",
    "    model.add(normalizer)\n",
    "    model.add(Conv1D(filters=32, kernel_size=3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model then build it to start training\n",
    "\n",
    "**Question** What is the purpose of the `shuffle=True` and `validation_split = 0.2` commands in the `model.fit` function? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "\n",
    "model.compile(optimizer='adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "EPOCHS = 64\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    epochs=EPOCHS,\n",
    "    validation_split = 0.2,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the accuracies and model losses obtained over training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learningCurve(history, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the models performance on the test set? Does this seem reasonable for this experiement? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(X_test, y_test)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_test_vec = np.argmax(y_test, axis=1)\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test_vec, y_pred)\n",
    "sns.heatmap(cf_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Alter the model\n",
    "- First, change the number of filters in the first layer to be size 64, and the kernal size to be 5. Observe how this alters the performance of the system.\n",
    "- Then, update the model again. Add in a new convolution, batch normalisation and max pooling layers. These should be added between the current max pooling and flattening layer. The size of the convolutional layer should be 32, the kernel size should be 3. The max pooling size should be 2 and stride 1. Observe how this alters the performance of the system.\n",
    "- Finally, Add ReLU activations to both convolutional layers. Observe how this alters the performance of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 5 Building a LSTM-RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we build our LSTM via a keras __[sequential](https://keras.io/guides/sequential_model/)__ model. \n",
    "\n",
    "**Question:** Describe the architecture of the model coded below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(n_timesteps,n_features)))\n",
    "    model.add(normalizer)\n",
    "    model.add(LSTM(32))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model then build it to start training\n",
    "\n",
    "**Question** Why has `shuffle` been set to `False` in the below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "\n",
    "model.compile(optimizer='adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "EPOCHS = 64\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    epochs=EPOCHS,\n",
    "    validation_split = 0.2,\n",
    "    shuffle=False,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the accuracies and model losses obtained over training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learningCurve(history, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(X_test, y_test)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_test_vec = np.argmax(y_test, axis=1)\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test_vec, y_pred)\n",
    "sns.heatmap(cf_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Alter the model\n",
    "- First, change the number of nodes in the first layer to be size 64. Then add in a new LSTM layer between the batch normalisation and dense layers. This layer should have 32 nodes. Perform Batch Normalisation after this layer as well. **Note**, in the first LSTM layer, you will need to set `return_sequences=True` so that the so that the second LSTM layer has a three-dimensional sequence input\n",
    "- Observe how this alters the performance of the system.\n",
    "- Then add this layer `model.add(Dropout(0.5))` in directly after both batch normalisation layers. What is the purpose of doing this? \n",
    "- Alter the code to run Gated Reccurent Units (GRUs) instead of LSTMs. Observe how this alters the performance of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement an __[early stopping](https://keras.io/api/callbacks/early_stopping/)__ strategy for the validation loss on both networks. \n",
    "\n",
    "**Note** the default `patience` value is 0. This means training is terminated as soon as the performance measure gets worse from one epoch to the next. This may not be the ideal value. \n",
    "\n",
    "**Note** don't run the `plot_learningCurve(history, EPOCHS)` function with early stopping, it will error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI4Health",
   "language": "python",
   "name": "ai4health"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "1925px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
