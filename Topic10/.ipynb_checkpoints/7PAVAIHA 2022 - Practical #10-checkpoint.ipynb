{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicle 10: Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this practical is to provide an overview of the main concepts of (Epsilon-Greedy) Q-learning,, a well-known approach to reinforcement learning, through two worked examples.\n",
    "\n",
    "The practical utilises the __[Open AI Gym](https://gym.openai.com)__ library, an open source toolkit for developing and comparing reinforcement learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-Up\n",
    "Install and import dependencies before starting the practical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Downloading gym-0.23.1.tar.gz (626 kB)\n",
      "\u001b[K     |████████████████████████████████| 626 kB 11.7 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gym-notices>=0.0.4\n",
      "  Downloading gym_notices-0.0.6-py3-none-any.whl (2.7 kB)\n",
      "Collecting cloudpickle>=1.2.0\n",
      "  Using cached cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0; python_version < \"3.10\" in /home/changhyun/kcl/AI4Health/ai4health/lib/python3.8/site-packages (from gym) (4.11.3)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/changhyun/kcl/AI4Health/ai4health/lib/python3.8/site-packages (from gym) (1.22.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/changhyun/kcl/AI4Health/ai4health/lib/python3.8/site-packages (from importlib-metadata>=4.10.0; python_version < \"3.10\"->gym) (3.8.0)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.23.1-py3-none-any.whl size=701355 sha256=8087c6ad18cdb50acf9e469d40e6c80b8b8f7155398d95636ed6ac190da9354d\n",
      "  Stored in directory: /home/changhyun/.cache/pip/wheels/78/28/77/b0c74e80a2a4faae0161d5c53bc4f8e436e77aedc79136ee13\n",
      "Successfully built gym\n",
      "Installing collected packages: gym-notices, cloudpickle, gym\n",
      "Successfully installed cloudpickle-2.0.0 gym-0.23.1 gym-notices-0.0.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygame\n",
      "  Downloading pygame-2.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 21.8 MB 10.6 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pygame\n",
      "Successfully installed pygame-2.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import envs\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from time import sleep\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function allows us to visualise short videos of the game ouput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 Self Driving Cab\n",
    "Our task is to implement Q-learning to run a simulation of a self-driving cab. In this task, there are 4 locations (labeled by different letters) and our job is to pick up the passenger at one location and drop them off in another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the environment\n",
    "\n",
    "First we create the Taxi-v3 environment.\n",
    "- It's a 5x5 grid world\n",
    "- Our Taxi is placed randomly in a square.\n",
    "- The passenger is placed randomly in one of the 4 possible location\n",
    "- They wish to go in one of the 4 possibles locations too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| :\u001b[43m \u001b[0m| : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "env.reset()\n",
    "print(env.render(mode = 'ansi'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the environment: \n",
    "- The filled square represents the taxi, which is yellow without a passenger and green with a passenger.\n",
    "- The pipe (\"|\") represents a wall which the taxi cannot cross.\n",
    "- R, G, Y, B are the possible pickup and destination locations. \n",
    "- The blue letter represents the current passenger pick-up location\n",
    "- The purple letter is the current destination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gameplay: \n",
    "\n",
    "**Actions:**\n",
    "There exactly six possible actions. With your taxi, you can try to move to the four cardinal directions: North, South, East or West. There are also two other possible actions which are the pick-up or the drop-off for a great total of six actions.\n",
    "\n",
    "The keys to use when playing the game are:\n",
    "- 0 = South (Down)\n",
    "- 1 = North (Up)\n",
    "- 2 = East (Right)\n",
    "- 3 = West (Left)\n",
    "- 4 = Pickup\n",
    "- 5 = Dropoff\n",
    "\n",
    "**Rewards**:\n",
    "We receive +20 points for a successful dropoff, and lose 1 point for every timestep it takes. \n",
    "There is also a 10 point penalty for illegal pick-up and drop-off actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing the game manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell allows you to play the game manually.\n",
    "- Can you follow the code? \n",
    "\n",
    "The code records the points earned, number of steps taken and total time taken to complete the game "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start counters at zero\n",
    "rew_tot = 0\n",
    "epochs = 0\n",
    "\n",
    "# Start a new game and print first screen\n",
    "print('Starting Position')\n",
    "new_state = env.reset()\n",
    "print(env.render(mode = 'ansi'))\n",
    "\n",
    "# The Open AI Gym function env.step() changes this variable to True when the game is solved\n",
    "done = False\n",
    "\n",
    "# For start the game timer\n",
    "start = time.time()\n",
    "\n",
    "# This is a list of all valid game commands \n",
    "valid_commands = ['0', '1', '2', '3', '4', '5', 'Q', 'q']\n",
    "\n",
    "while done != True: # Keep playing the game while 'Done' is not set to True\n",
    "    \n",
    "    action = input('Enter your next move:') #prints the text box to enter the game instructions\n",
    "    \n",
    "    if action=='Q' or action=='q': #Entering Q or q quits the game \n",
    "        break\n",
    "        \n",
    "    elif action in valid_commands: # makes sure a valid command is entered\n",
    "        \n",
    "        # env.step() takes in an action and returns the next state and reward for the environment\n",
    "        # It also set the 'Done' variable to 'True' if:\n",
    "            #(i) we complete the game\n",
    "            # (ii) we have taken over 200 epochs and not completed the game      \n",
    "        new_state, reward, done, info = env.step(int(action)) \n",
    "        \n",
    "        # Update rewards and epochs\n",
    "        rew_tot += reward  \n",
    "        epochs += 1\n",
    "        \n",
    "        # Print game state and current reward total\n",
    "        clear_output(wait=True)\n",
    "        print(env.render(mode = 'ansi'))\n",
    "        print(\"Reward: %r\" % rew_tot)\n",
    "          \n",
    "    else: # Print warning if command not valide\n",
    "        \n",
    "        print('Incorrect Command, Please Try Again!')\n",
    "\n",
    "# while loop has ended as env.step() has set 'Done = True'\n",
    "\n",
    "#Stop the timer\n",
    "end = time.time()\n",
    "\n",
    "#Print the final game state and scores \n",
    "clear_output(wait=True)\n",
    "print(\"Game Over:\")\n",
    "print(env.render(mode = 'ansi'))\n",
    "print(\"Final Reward: %r\" % rew_tot)\n",
    "print(\"Epochs taken: {}\".format(epochs))\n",
    "print(\"Time taken: {:.5f} seconds\".format(end - start))\n",
    "\n",
    "# Reminder of the controls\n",
    "# 0 = South (Down)\n",
    "# 1 = North (Up)\n",
    "# 2 = East (Right)\n",
    "# 3 = West (Left)\n",
    "# 4 = Pickup\n",
    "# 5 = Dropoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the game randomly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell randomly samples the action space to solve the game. \n",
    "\n",
    "The code records the points earned, number of steps taken,and total time taken to complete the game "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start counters at zero\n",
    "rew_tot = 0\n",
    "epochs = 0\n",
    "\n",
    "# set this to blank to all animation of game once completed\n",
    "frames = []\n",
    "\n",
    "# Start a new game and print first screen\n",
    "new_state = env.reset()\n",
    "print('Starting Position')\n",
    "print(env.render(mode = 'ansi'))\n",
    "\n",
    "# The Open AI Gym function env.step() changes this variable to True when the game is solved\n",
    "done = False\n",
    "\n",
    "# For start the game timer\n",
    "start = time.time()\n",
    "\n",
    "while done != True:\n",
    "    \n",
    "    # Randomly sample an action from the game\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # play the game and get new variables \n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    \n",
    "    # Update rewards and epochs\n",
    "    rew_tot += reward  \n",
    "    epochs += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': new_state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        })\n",
    "\n",
    "# Game Over \n",
    "\n",
    "#Stop the timer\n",
    "end = time.time()\n",
    "\n",
    "#Print the final game state and scores \n",
    "print(\"Game Over\")\n",
    "print(env.render(mode = 'ansi'))\n",
    "print(\"Final Reward: %r\" % rew_tot)\n",
    "print(\"Epochs taken: {}\".format(epochs))\n",
    "print(\"Time taken: {:.5f} seconds\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the game should have stopped with a very negative points total and will have timed out; e.g., `Epochs taken` is set to `200`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Watch the game:**\n",
    "\n",
    "You can now call the `print_frames` function to watch the gameplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the game through Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Q-learning we create and fill a table (`Q-Table`) storing state-action pairs. \n",
    "\n",
    "During train we fill the `Q-Table` with the maximum expected future rewards for action at each state. \n",
    "\n",
    "After training, the `Q-Table` serves as a guide to the best action at each state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 Create the Q-table and initialize it\n",
    "Now, we need to create our Q-table. To do this we need to calculate the number of rows (states) and columns (actions). To do this we need to know the action_space and the observation_space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action Space** \n",
    "\n",
    "Reminder, the game has six possible actions: North, South, East or West, and Pick-up or Drop-off \n",
    "\n",
    "**Obsevation Space** \n",
    "\n",
    "We can break up the parking lot into a 5x5 grid, which gives us 25 possible taxi locations. These 25 locations are one part of our state space. \n",
    "\n",
    "**All game states:**\n",
    "\n",
    "We also need to take all combinations of passenger locations and destination locations to come to a total number of states for our taxi environment; there's four (4) destinations {R, G, Y, B} and five {R, G, Y, B, in cab} passenger locations.\n",
    "\n",
    "So, our taxi environment has 5×5×4×5=500 total possible states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Gym provides us a way to do that: `env.action_space.n` and `env.observation_space.n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = env.observation_space.n\n",
    "print(\"There are \", state_space, \" possible states\")\n",
    "action_space = env.action_space.n\n",
    "print(\"There are \", action_space, \" possible actions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need to create a Q table with state_size rows and action_size columns (500x6)\n",
    "\n",
    "We do this using the zeros function in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_Table = np.zeros((state_space, action_space))\n",
    "print(Q_Table)\n",
    "print(Q_Table.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 Define the hyperparameters\n",
    "- **Question** What is the role of all of these parameters in Q-learning? \n",
    "- **Question** Do these values seem reasonable?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 500  # Total number of training episodes\n",
    "alpha = 0.5           # Learning rate\n",
    "gamma = 0.5           # Discounting rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Train the Q-Table\n",
    "\n",
    "Make sure you can follow each step in the code.\n",
    "\n",
    "- **Question** What is the name of the equation we are using to updated (train) the Q-Table? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise a blank Q-Table\n",
    "Q_Table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "# Set Hyperparameters\n",
    "total_episodes = 500  # Total number of training episodes\n",
    "alpha = 0.5           # Learning rate\n",
    "gamma = 0.5           # Discounting rate\n",
    "\n",
    "# Traing looop\n",
    "for episode in range(total_episodes+1):\n",
    "    \n",
    "    Train_done = False\n",
    "    state = env.reset()\n",
    "    \n",
    "    while Train_done != True:\n",
    " \n",
    "        #choosing the action with the highest Q value\n",
    "        action = np.argmax(Q_Table[state])  \n",
    "\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, Train_done, info = env.step(action)\n",
    "        \n",
    "        # Update The Q-TabeL:\n",
    "        # Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q_Table(s',a') - Q_Table(s,a)]\n",
    "        Q_Table[state,action] = Q_Table[state][action] + alpha * (reward + gamma * \n",
    "                                    np.max(Q_Table[new_state]) - Q_Table[state,action])   \n",
    "        # Update state\n",
    "        state = new_state\n",
    "        \n",
    "    # Valid the performance of the Q-Table every 100 training episodes \n",
    "    if episode % 100 == 0:\n",
    "        #reset the average validation accurcy\n",
    "        average_reward = 0.\n",
    "        # Test the performance of the Q_Table 100 times\n",
    "        for i in range(100):\n",
    "\n",
    "            state = env.reset()\n",
    "            Validation_done = False\n",
    "\n",
    "            while Validation_done != True: \n",
    "\n",
    "                action = np.argmax(Q_Table[state])\n",
    "                state, reward, Validation_done, info = env.step(action)\n",
    "                average_reward += reward\n",
    "\n",
    "        # Validation Loop Finished       \n",
    "        average_reward = average_reward/100\n",
    "        clear_output(wait=True)\n",
    "        print('Episode {} avarage reward: {}'.format(episode,average_reward)) \n",
    "            \n",
    "# Training finished\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Play the game utilising the Q-Table to determine the next step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next block of code demonstrates high to solve the game usingthe Q-Table\n",
    "- **Question** do you understand how this is done in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [] # for animation\n",
    "\n",
    "rew_tot = 0\n",
    "epochs = 0\n",
    "done = False\n",
    "\n",
    "state = env.reset()\n",
    "print('Starting Position')\n",
    "print(env.render(mode = 'ansi'))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    #Choose action and update state\n",
    "    action = np.argmax(Q_Table[state])  \n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    \n",
    "    #Track rewards and epochs\n",
    "    rew_tot = rew_tot + reward\n",
    "    epochs += 1\n",
    "    \n",
    "    # Update state\n",
    "    state = new_state\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': new_state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Game Over: Final Reward: %r\" % rew_tot)\n",
    "print(env.render(mode = 'ansi'))\n",
    "print(\"Time taken: {:.5f} seconds\".format(end - start))\n",
    "print(\"Epochs taken: {}\".format(epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `print_frames` to watch the gameplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check avearge performance** To get a better understanding of the how good our Q-Table is determine the avearge game metrics over 5000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_reward = 0\n",
    "average_epochs = 0\n",
    "average_time = 0\n",
    "\n",
    "for i in range(5000):\n",
    "    \n",
    "    rew_tot = 0\n",
    "    epochs = 0\n",
    "    done = False\n",
    "    \n",
    "    state = env.reset()\n",
    "\n",
    "    start = time.time()\n",
    "    while not done:\n",
    "\n",
    "        #Choose action and update state\n",
    "        action = np.argmax(Q_Table[state])  \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        #Track rewards and epochs\n",
    "        rew_tot = rew_tot + reward\n",
    "        epochs += 1\n",
    "\n",
    "        # Update state\n",
    "        state = new_state\n",
    "        \n",
    "    # While loop is over\n",
    "    end = time.time()\n",
    "    average_reward += rew_tot\n",
    "    average_epochs += epochs\n",
    "    average_time += (end - start)\n",
    "    \n",
    "# Loop Finished \n",
    "print(\"Average Reward: {}\".format(average_reward/5000))\n",
    "print(\"Time taken: {:.5f} seconds\".format(average_time/5000))\n",
    "print(\"Epochs taken: {}\".format(average_epochs/5000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise Improve the reinforcment learning algorithm\n",
    "\n",
    "As you can see, our AI agent is not very intellegent. \n",
    "\n",
    "See if you can improve it's performance through the tuning of the hyperparameters\n",
    "\n",
    "Additionally, yse the validation loop to implement an early stoping criteria which stops training when avearge reward validation reward reaches a value between greater than 5. *Hint* This can be down in two additional lines of code.\n",
    "\n",
    "Use the aveage performance code to verify the improved performance of your table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Frozen Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the game\n",
    "\n",
    "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake8x8-v1')\n",
    "env.reset()\n",
    "print(env.render(mode = 'ansi'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the environment: \n",
    "The surface is described using a grid like the following:\n",
    "- S: starting point, safe\n",
    "- F: frozen surface, safe\n",
    "- H: hole, fall to your doom\n",
    "- G: goal, where the frisbee is located"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gameplay: \n",
    "\n",
    "**Actions:** The commands to  nativagite the enviroment are\n",
    "- 0 = Left\n",
    "- 1 = Down\n",
    "- 2 = Right\n",
    "- 3 = Up\n",
    "\n",
    "As ice is slippery, there is only a 1/3 chance of moving in the chosen direction. These is also 1/3 chance to move in either direction perpendicular to the intended direction. \n",
    "*For example*, if action is `left` and, then:\n",
    "- P(move left)=1/3\n",
    "- P(move up)=1/3\n",
    "- P(move down)=1/3\n",
    "\n",
    "**Rewards:** The reward schedule is :\n",
    "- Reach goal (G): +1\n",
    "- Reach hole (H): 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: \n",
    "Implement Q-learning to run a simulation of the frozen lake game\n",
    "\n",
    "**Hint** \n",
    "Some reasonable hyperparamters for this game are\n",
    "- Nuber of training episodes: 500000\n",
    "- Discount factor: 0.95\n",
    "- Learning rate: 0.01\n",
    "- Perform the validation loop every 10000 episodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observe** the performance of the Q-table one iteration of the game\n",
    "- What is happening? **Answer** The agent doesn't much\n",
    "- Why do you think this is? **Answer** The is no insentive to move, there is no punishment for not moving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [] # for animation\n",
    "\n",
    "rew_tot = 0\n",
    "epochs = 0\n",
    "new_state = env.reset()\n",
    "print(env.render(mode = 'ansi'))\n",
    "done = False\n",
    "\n",
    "start = time.time()\n",
    "while not done:\n",
    "    \n",
    "    action = np.argmax(Q_Table[new_state])  \n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    rew_tot = rew_tot + reward  \n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': new_state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "print(\"Game Over: Final Reward: %r\" % rew_tot)\n",
    "print(\"Time taken: {:.5f} seconds\".format(end - start))\n",
    "print(\"Epochs taken: {}\".format(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "To improve the performance of our training you will need to implement an __[epsilon-greedy](https://www.baeldung.com/cs/epsilon-greedy-q-learning)__ policy to encourage more expolration of the environment\n",
    "- Hint use `if: else:` statement as part of your solution\n",
    "\n",
    "Additionally include an early stopping policy to quit updating the table once the average validation reward is 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [] # for animation\n",
    "\n",
    "rew_tot = 0\n",
    "epochs = 0\n",
    "\n",
    "new_state = env.reset()\n",
    "print(\"Game Start:\")\n",
    "print(env.render(mode = 'ansi'))\n",
    "\n",
    "done = False\n",
    "\n",
    "start = time.time()\n",
    "while not done:\n",
    "    \n",
    "    action = np.argmax(Q_Table[new_state])  \n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    rew_tot = rew_tot + reward  \n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': new_state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "print(\"Game Over: Final Reward: %r\" % rew_tot)\n",
    "print(env.render(mode = 'ansi'))\n",
    "print(\"Time taken: {:.5f} seconds\".format(end - start))\n",
    "print(\"Epochs taken: {}\".format(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4health",
   "language": "python",
   "name": "ai4health"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
