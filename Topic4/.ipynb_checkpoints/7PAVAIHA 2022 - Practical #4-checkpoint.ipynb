{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 4: Particle Swarm Optimisation for Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In today's lab, we will learn how to perform feature selection using particle swarm optimistation, utilising the __[pyswarms](https://pyswarms.readthedocs.io/en/latest/index.html)__ toolkit.\n",
    "\n",
    "\n",
    "The aim of feature selection is to automatically select suitable data features suitable for use in AI models and algorithms. This is an important process before model training (covered in machine learning and deep learning parts of the course) as too many or redundant features can negatively impact the learning and accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install and importing relevant modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyswarms\n",
      "  Downloading pyswarms-1.3.0-py2.py3-none-any.whl (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 3.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=1.3.1 in /home/changhyun/kcl/ml_bioinfo_hi/ml_health_bio/lib/python3.8/site-packages (from pyswarms) (3.5.1)\n",
      "Requirement already satisfied: tqdm in /home/changhyun/kcl/ml_bioinfo_hi/ml_health_bio/lib/python3.8/site-packages (from pyswarms) (4.63.0)\n",
      "Requirement already satisfied: attrs in /home/changhyun/kcl/ml_bioinfo_hi/ml_health_bio/lib/python3.8/site-packages (from pyswarms) (21.4.0)\n",
      "Requirement already satisfied: numpy in /home/changhyun/kcl/ml_bioinfo_hi/ml_health_bio/lib/python3.8/site-packages (from pyswarms) (1.21.0)\n",
      "Collecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[K     |████████████████████████████████| 829 kB 5.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyyaml\n",
      "  Using cached PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
      "Requirement already satisfied: scipy in /home/changhyun/kcl/ml_bioinfo_hi/ml_health_bio/lib/python3.8/site-packages (from pyswarms) (1.8.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/changhyun/kcl/ml_bioinfo_hi/ml_health_bio/lib/python3.8/site-packages (from matplotlib>=1.3.1->pyswarms) (9.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/changhyun/kcl/ml_bioinfo_hi/ml_health_bio/lib/python3.8/site-packages (from matplotlib>=1.3.1->pyswarms) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/changhyun/kcl/ml_bioinfo_hi/ml_health_bio/lib/python3.8/site-packages (from matplotlib>=1.3.1->pyswarms) (1.4.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/changhyun/kcl/ml_bioinfo_hi/ml_health_bio/lib/python3.8/site-packages (from matplotlib>=1.3.1->pyswarms) (4.31.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/changhyun/kcl/ml_bioinfo_hi/ml_health_bio/lib/python3.8/site-packages (from matplotlib>=1.3.1->pyswarms) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/changhyun/kcl/ml_bioinfo_hi/ml_health_bio/lib/python3.8/site-packages (from matplotlib>=1.3.1->pyswarms) (3.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/changhyun/kcl/ml_bioinfo_hi/ml_health_bio/lib/python3.8/site-packages (from matplotlib>=1.3.1->pyswarms) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/changhyun/kcl/ml_bioinfo_hi/ml_health_bio/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib>=1.3.1->pyswarms) (1.16.0)\n",
      "Building wheels for collected packages: future\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491058 sha256=1b883fcd5e71d697ed3fcbc55bffa456c3901b33c35ec6a87c77db76e5bf6a83\n",
      "  Stored in directory: /home/changhyun/.cache/pip/wheels/8e/70/28/3d6ccd6e315f65f245da085482a2e1c7d14b90b30f239e2cf4\n",
      "Successfully built future\n",
      "Installing collected packages: future, pyyaml, pyswarms\n",
      "Successfully installed future-0.18.2 pyswarms-1.3.0 pyyaml-6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyswarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pyswarms as ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create dataset with redundant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use sklearn's `make_classification` function to create a dataset with a large number of redundant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of features for each type\n",
    "n_features = 200\n",
    "n_informative = 20\n",
    "n_redundant = 100\n",
    "n_repeated = 50\n",
    "n_useless = 30\n",
    "\n",
    "# Create Labels\n",
    "informative_labels = [f'Informative {ii}' for ii in range(1, n_informative + 1)]\n",
    "redundant_labels = [f'Redundant {ii}' for ii in range(n_informative + 1, n_informative + n_redundant + 1)]\n",
    "repeated_labels = [f'Repeated {ii}' for ii in range(n_informative + n_redundant+ 1, n_informative + n_redundant + n_repeated + 1)]\n",
    "useless_labels = [f'Useless {ii}' for ii in range(n_informative + n_redundant + n_repeated + 1, n_features + 1)]\n",
    "labels = informative_labels + redundant_labels + repeated_labels + useless_labels\n",
    "\n",
    "# Get data\n",
    "X, y = make_classification(n_samples = 5000, n_features = n_features,\n",
    "                           n_informative = n_informative,\n",
    "                           n_redundant = n_redundant , n_repeated = n_repeated,\n",
    "                           n_clusters_per_class = 2, class_sep = 0.5, flip_y = 0.05,\n",
    "                           random_state = 42, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In role of the code in the next cell block is to standardise the data\n",
    "\n",
    "**Question** What does this do to the data, and why might this be important for feature selection?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Set-up the objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll be using the optimizer `pyswarms.discrete.BinaryPSO` to perform feature subset selection. \n",
    "\n",
    "For a Binary PSO, the position of the particles are expressed in two terms: 1 or 0 (or on and off). Mathematically this is defined as: \n",
    "                    $x=[x_{1},x_{2},x_{3},…,x_{d}]$   where   $x_{i}\\in {0,1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective function we will be using is taken from this paper and is define as:\n",
    "\n",
    "$f(X)=\\alpha(1−P)+(1−\\alpha)(1− \\dfrac{N_{f}}{N_{t}})$\n",
    "\n",
    "Where $\\alpha$ is a hyperparameter tradeoffs the performance of classifier $P$, with the ratio of the size of the feature subset $N_{f}$ with respect to the total number of features $N_{t}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First** we need to define a function which calcualtes the objective function per particle. \n",
    "\n",
    "It return the object loss score for a *single* particle\n",
    "\n",
    " Do you understand the purpose of line in the function? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Computes for the objective function per particle\n",
    "\n",
    "Inputs\n",
    "------\n",
    "m : numpy.ndarray\n",
    "    Binary mask that can be obtained from BinaryPSO, will\n",
    "    be used to mask features.\n",
    "alpha: float (default is 0.5)\n",
    "    Constant weight for trading-off classifier performance\n",
    "    and number of features\n",
    "\n",
    "Returns\n",
    "-------\n",
    "numpy.ndarray\n",
    "    Computed objective function\n",
    "\"\"\"\n",
    "\n",
    "# Create an instance of the classifier\n",
    "classifier = RandomForestClassifier(max_depth=2,n_estimators=10)\n",
    "\n",
    "# Define objective function\n",
    "def f_per_particle(m, alpha):\n",
    "\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "    \n",
    "    # Get the subset of the features from the binary mask\n",
    "    if np.count_nonzero(m) == 0:\n",
    "        X_train_subset = X_train\n",
    "        X_test_subset = X_test\n",
    "    else:\n",
    "        X_train_subset = X_train[:,m==1]\n",
    "        X_test_subset = X_test[:,m==1]\n",
    "        \n",
    "    # Perform classification and store performance in P\n",
    "    classifier.fit(X_train_subset, y_train)\n",
    "    P = (classifier.predict(X_test_subset) == y_test).mean()\n",
    "\n",
    "    # Compute for the objective function\n",
    "    j = (alpha * (1.0 - P)\n",
    "        + (1.0 - alpha) * (1 - (X_train_subset.shape[1] /X_train.shape[1])))\n",
    "    \n",
    "    return j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next** we define the higher level objective function to be evaluated in pyswarms built in optimiser. \n",
    "\n",
    "It returns the object loss score for *all* particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Higher-level method to do classification in the\n",
    "whole swarm.\n",
    "\n",
    "Inputs\n",
    "------\n",
    "M: numpy.ndarray of shape (n_particles, dimensions)\n",
    "    The swarm that will perform the search\n",
    "\n",
    "Returns\n",
    "-------\n",
    "numpy.ndarray of shape (n_particles, )\n",
    "    The computed loss for each particle\n",
    "\"\"\"\n",
    "\n",
    "def f(M, alpha=0.5):\n",
    "\n",
    "    n_particles = M.shape[0]\n",
    "    \n",
    "    j = [f_per_particle(M[i], alpha) for i in range(n_particles)]\n",
    "\n",
    "    return np.array(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Set-up the optimisation parameters and run the selections algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `options` dictionary below: \n",
    "- `k`represents the neighbors to be considered when calculating the best known position of the swarm\n",
    "- `p`represents a distance metric used in the optimisation algorithm. \n",
    "\n",
    "**Question** What are the purpose of the other three parameters? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize swarm, arbitrary\n",
    "options = {'c1': 0.5, 'c2': 0.5, 'w':0.9, 'k': 2, 'p':2}\n",
    "\n",
    "# Call instance of PSO\n",
    "dimensions = X.shape[1] # dimensions should be the number of features\n",
    "\n",
    "optimizer = ps.discrete.BinaryPSO(n_particles=60, dimensions=dimensions, options=options)\n",
    "\n",
    "# Perform optimization\n",
    "tic = time.perf_counter()\n",
    "cost, pos = optimizer.optimize(f, iters=20, verbose=2)\n",
    "toc = time.perf_counter()\n",
    "print(f\"Optimiser ran for {toc - tic:0.4f} seconds\")\n",
    "selected = np.count_nonzero(pos)\n",
    "print(\"Optimiser retained \" + str(selected)   + \" features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evalute the performamce of the selected features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the effectiveness of the selected features, we can use two Random Forest Classifiers, one trained on the full feature set, the other trained on the selected feature sets, and compare the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "c1 = RandomForestClassifier(max_depth=2, n_estimators=10, random_state=42)\n",
    "c1.fit(X_train, y_train)\n",
    "full_performance = (c1.predict(X_test) == y_test).mean()\n",
    "print('Full Feature set performance: %.3f' % (full_performance))\n",
    "\n",
    "c2 = RandomForestClassifier(max_depth=2, n_estimators=10, random_state=42)\n",
    "c2.fit(X_train[:,pos==1], y_train)\n",
    "subset_performance = (c2.predict(X_test[:,pos==1]) == y_test).mean()\n",
    "print('Subset Feature set performance: %.3f' % (subset_performance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how altering key parameters changes the performance and running time of the optimiser.\n",
    "\n",
    "Parameters to consider altering include\n",
    "- alpha\n",
    "- Swarm options\n",
    "- Number of particles\n",
    "- Number of optimiser iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement feature selection on the *Breast Cancer Wisconsin Diagnostic Dataset*. The features present in this dataset are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. The features are labelled as coming from either a malignant or benign sample.\n",
    "\n",
    "The data can be imported directly from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer()\n",
    "print(data.DESCR) \n",
    "\n",
    "X, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "174px",
    "left": "1897px",
    "right": "20px",
    "top": "303px",
    "width": "467px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
