{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 4: Particle Swarm Optimisation for Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In today's lab, we will learn how to perform feature selection using particle swarm optimistation, utilising the __[pyswarms](https://pyswarms.readthedocs.io/en/latest/index.html)__ toolkit.\n",
    "\n",
    "\n",
    "The aim of feature selection is to automatically select suitable data features suitable for use in AI models and algorithms. This is an important process before model training (covered in machine learning and deep learning parts of the course) as too many or redundant features can negatively impact the learning and accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install and importing relevant modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyswarms\n",
      "  Downloading pyswarms-1.3.0-py2.py3-none-any.whl (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 3.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=1.3.1 in /home/changhyun/kcl/ml_bioinfo_hi/ml_health_bio/lib/python3.8/site-packages (from pyswarms) (3.5.1)\n",
      "Requirement already satisfied: tqdm in /home/changhyun/kcl/ml_bioinfo_hi/ml_health_bio/lib/python3.8/site-packages (from pyswarms) (4.63.0)\n",
      "Requirement already satisfied: attrs in /home/changhyun/kcl/ml_bioinfo_hi/ml_health_bio/lib/python3.8/site-packages (from pyswarms) (21.4.0)\n",
      "Requirement already satisfied: numpy in /home/changhyun/kcl/ml_bioinfo_hi/ml_health_bio/lib/python3.8/site-packages (from pyswarms) (1.21.0)\n",
      "Collecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[K     |████████████████████████████████| 829 kB 5.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyyaml\n",
      "  Using cached PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
      "Requirement already satisfied: scipy in /home/changhyun/kcl/ml_bioinfo_hi/ml_health_bio/lib/python3.8/site-packages (from pyswarms) (1.8.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/changhyun/kcl/ml_bioinfo_hi/ml_health_bio/lib/python3.8/site-packages (from matplotlib>=1.3.1->pyswarms) (9.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/changhyun/kcl/ml_bioinfo_hi/ml_health_bio/lib/python3.8/site-packages (from matplotlib>=1.3.1->pyswarms) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/changhyun/kcl/ml_bioinfo_hi/ml_health_bio/lib/python3.8/site-packages (from matplotlib>=1.3.1->pyswarms) (1.4.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/changhyun/kcl/ml_bioinfo_hi/ml_health_bio/lib/python3.8/site-packages (from matplotlib>=1.3.1->pyswarms) (4.31.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/changhyun/kcl/ml_bioinfo_hi/ml_health_bio/lib/python3.8/site-packages (from matplotlib>=1.3.1->pyswarms) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/changhyun/kcl/ml_bioinfo_hi/ml_health_bio/lib/python3.8/site-packages (from matplotlib>=1.3.1->pyswarms) (3.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/changhyun/kcl/ml_bioinfo_hi/ml_health_bio/lib/python3.8/site-packages (from matplotlib>=1.3.1->pyswarms) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/changhyun/kcl/ml_bioinfo_hi/ml_health_bio/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib>=1.3.1->pyswarms) (1.16.0)\n",
      "Building wheels for collected packages: future\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491058 sha256=1b883fcd5e71d697ed3fcbc55bffa456c3901b33c35ec6a87c77db76e5bf6a83\n",
      "  Stored in directory: /home/changhyun/.cache/pip/wheels/8e/70/28/3d6ccd6e315f65f245da085482a2e1c7d14b90b30f239e2cf4\n",
      "Successfully built future\n",
      "Installing collected packages: future, pyyaml, pyswarms\n",
      "Successfully installed future-0.18.2 pyswarms-1.3.0 pyyaml-6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyswarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pyswarms as ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create dataset with redundant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use sklearn's `make_classification` function to create a dataset with a large number of redundant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of features for each type\n",
    "n_features = 200\n",
    "n_informative = 20\n",
    "n_redundant = 100\n",
    "n_repeated = 50\n",
    "n_useless = 30\n",
    "\n",
    "# Create Labels\n",
    "informative_labels = [f'Informative {ii}' for ii in range(1, n_informative + 1)]\n",
    "redundant_labels = [f'Redundant {ii}' for ii in range(n_informative + 1, n_informative + n_redundant + 1)]\n",
    "repeated_labels = [f'Repeated {ii}' for ii in range(n_informative + n_redundant+ 1, n_informative + n_redundant + n_repeated + 1)]\n",
    "useless_labels = [f'Useless {ii}' for ii in range(n_informative + n_redundant + n_repeated + 1, n_features + 1)]\n",
    "labels = informative_labels + redundant_labels + repeated_labels + useless_labels\n",
    "\n",
    "# Get data\n",
    "X, y = make_classification(n_samples = 5000, n_features = n_features,\n",
    "                           n_informative = n_informative,\n",
    "                           n_redundant = n_redundant , n_repeated = n_repeated,\n",
    "                           n_clusters_per_class = 2, class_sep = 0.5, flip_y = 0.05,\n",
    "                           random_state = 42, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Informative 1',\n",
       " 'Informative 2',\n",
       " 'Informative 3',\n",
       " 'Informative 4',\n",
       " 'Informative 5',\n",
       " 'Informative 6',\n",
       " 'Informative 7',\n",
       " 'Informative 8',\n",
       " 'Informative 9',\n",
       " 'Informative 10',\n",
       " 'Informative 11',\n",
       " 'Informative 12',\n",
       " 'Informative 13',\n",
       " 'Informative 14',\n",
       " 'Informative 15',\n",
       " 'Informative 16',\n",
       " 'Informative 17',\n",
       " 'Informative 18',\n",
       " 'Informative 19',\n",
       " 'Informative 20',\n",
       " 'Redundant 21',\n",
       " 'Redundant 22',\n",
       " 'Redundant 23',\n",
       " 'Redundant 24',\n",
       " 'Redundant 25',\n",
       " 'Redundant 26',\n",
       " 'Redundant 27',\n",
       " 'Redundant 28',\n",
       " 'Redundant 29',\n",
       " 'Redundant 30',\n",
       " 'Redundant 31',\n",
       " 'Redundant 32',\n",
       " 'Redundant 33',\n",
       " 'Redundant 34',\n",
       " 'Redundant 35',\n",
       " 'Redundant 36',\n",
       " 'Redundant 37',\n",
       " 'Redundant 38',\n",
       " 'Redundant 39',\n",
       " 'Redundant 40',\n",
       " 'Redundant 41',\n",
       " 'Redundant 42',\n",
       " 'Redundant 43',\n",
       " 'Redundant 44',\n",
       " 'Redundant 45',\n",
       " 'Redundant 46',\n",
       " 'Redundant 47',\n",
       " 'Redundant 48',\n",
       " 'Redundant 49',\n",
       " 'Redundant 50',\n",
       " 'Redundant 51',\n",
       " 'Redundant 52',\n",
       " 'Redundant 53',\n",
       " 'Redundant 54',\n",
       " 'Redundant 55',\n",
       " 'Redundant 56',\n",
       " 'Redundant 57',\n",
       " 'Redundant 58',\n",
       " 'Redundant 59',\n",
       " 'Redundant 60',\n",
       " 'Redundant 61',\n",
       " 'Redundant 62',\n",
       " 'Redundant 63',\n",
       " 'Redundant 64',\n",
       " 'Redundant 65',\n",
       " 'Redundant 66',\n",
       " 'Redundant 67',\n",
       " 'Redundant 68',\n",
       " 'Redundant 69',\n",
       " 'Redundant 70',\n",
       " 'Redundant 71',\n",
       " 'Redundant 72',\n",
       " 'Redundant 73',\n",
       " 'Redundant 74',\n",
       " 'Redundant 75',\n",
       " 'Redundant 76',\n",
       " 'Redundant 77',\n",
       " 'Redundant 78',\n",
       " 'Redundant 79',\n",
       " 'Redundant 80',\n",
       " 'Redundant 81',\n",
       " 'Redundant 82',\n",
       " 'Redundant 83',\n",
       " 'Redundant 84',\n",
       " 'Redundant 85',\n",
       " 'Redundant 86',\n",
       " 'Redundant 87',\n",
       " 'Redundant 88',\n",
       " 'Redundant 89',\n",
       " 'Redundant 90',\n",
       " 'Redundant 91',\n",
       " 'Redundant 92',\n",
       " 'Redundant 93',\n",
       " 'Redundant 94',\n",
       " 'Redundant 95',\n",
       " 'Redundant 96',\n",
       " 'Redundant 97',\n",
       " 'Redundant 98',\n",
       " 'Redundant 99',\n",
       " 'Redundant 100',\n",
       " 'Redundant 101',\n",
       " 'Redundant 102',\n",
       " 'Redundant 103',\n",
       " 'Redundant 104',\n",
       " 'Redundant 105',\n",
       " 'Redundant 106',\n",
       " 'Redundant 107',\n",
       " 'Redundant 108',\n",
       " 'Redundant 109',\n",
       " 'Redundant 110',\n",
       " 'Redundant 111',\n",
       " 'Redundant 112',\n",
       " 'Redundant 113',\n",
       " 'Redundant 114',\n",
       " 'Redundant 115',\n",
       " 'Redundant 116',\n",
       " 'Redundant 117',\n",
       " 'Redundant 118',\n",
       " 'Redundant 119',\n",
       " 'Redundant 120',\n",
       " 'Repeated 121',\n",
       " 'Repeated 122',\n",
       " 'Repeated 123',\n",
       " 'Repeated 124',\n",
       " 'Repeated 125',\n",
       " 'Repeated 126',\n",
       " 'Repeated 127',\n",
       " 'Repeated 128',\n",
       " 'Repeated 129',\n",
       " 'Repeated 130',\n",
       " 'Repeated 131',\n",
       " 'Repeated 132',\n",
       " 'Repeated 133',\n",
       " 'Repeated 134',\n",
       " 'Repeated 135',\n",
       " 'Repeated 136',\n",
       " 'Repeated 137',\n",
       " 'Repeated 138',\n",
       " 'Repeated 139',\n",
       " 'Repeated 140',\n",
       " 'Repeated 141',\n",
       " 'Repeated 142',\n",
       " 'Repeated 143',\n",
       " 'Repeated 144',\n",
       " 'Repeated 145',\n",
       " 'Repeated 146',\n",
       " 'Repeated 147',\n",
       " 'Repeated 148',\n",
       " 'Repeated 149',\n",
       " 'Repeated 150',\n",
       " 'Repeated 151',\n",
       " 'Repeated 152',\n",
       " 'Repeated 153',\n",
       " 'Repeated 154',\n",
       " 'Repeated 155',\n",
       " 'Repeated 156',\n",
       " 'Repeated 157',\n",
       " 'Repeated 158',\n",
       " 'Repeated 159',\n",
       " 'Repeated 160',\n",
       " 'Repeated 161',\n",
       " 'Repeated 162',\n",
       " 'Repeated 163',\n",
       " 'Repeated 164',\n",
       " 'Repeated 165',\n",
       " 'Repeated 166',\n",
       " 'Repeated 167',\n",
       " 'Repeated 168',\n",
       " 'Repeated 169',\n",
       " 'Repeated 170',\n",
       " 'Useless 171',\n",
       " 'Useless 172',\n",
       " 'Useless 173',\n",
       " 'Useless 174',\n",
       " 'Useless 175',\n",
       " 'Useless 176',\n",
       " 'Useless 177',\n",
       " 'Useless 178',\n",
       " 'Useless 179',\n",
       " 'Useless 180',\n",
       " 'Useless 181',\n",
       " 'Useless 182',\n",
       " 'Useless 183',\n",
       " 'Useless 184',\n",
       " 'Useless 185',\n",
       " 'Useless 186',\n",
       " 'Useless 187',\n",
       " 'Useless 188',\n",
       " 'Useless 189',\n",
       " 'Useless 190',\n",
       " 'Useless 191',\n",
       " 'Useless 192',\n",
       " 'Useless 193',\n",
       " 'Useless 194',\n",
       " 'Useless 195',\n",
       " 'Useless 196',\n",
       " 'Useless 197',\n",
       " 'Useless 198',\n",
       " 'Useless 199',\n",
       " 'Useless 200']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In role of the code in the next cell block is to standardise the data\n",
    "\n",
    "**Question** What does this do to the data, and why might this be important for feature selection?\n",
    "\n",
    "It is to scale data into a boundary that prevents extremely large or small values to be dominant during model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Set-up the objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll be using the optimizer `pyswarms.discrete.BinaryPSO` to perform feature subset selection. \n",
    "\n",
    "For a Binary PSO, the position of the particles are expressed in two terms: 1 or 0 (or on and off). Mathematically this is defined as: \n",
    "                    $x=[x_{1},x_{2},x_{3},…,x_{d}]$   where   $x_{i}\\in {0,1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective function we will be using is taken from this paper and is define as:\n",
    "\n",
    "$f(X)=\\alpha(1−P)+(1−\\alpha)(1− \\dfrac{N_{f}}{N_{t}})$\n",
    "\n",
    "Where $\\alpha$ is a hyperparameter tradeoffs the performance of classifier $P$, with the ratio of the size of the feature subset $N_{f}$ with respect to the total number of features $N_{t}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First** we need to define a function which calcualtes the objective function per particle. \n",
    "\n",
    "It return the object loss score for a *single* particle\n",
    "\n",
    " Do you understand the purpose of line in the function? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Computes for the objective function per particle\n",
    "\n",
    "Inputs\n",
    "------\n",
    "m : numpy.ndarray\n",
    "    Binary mask that can be obtained from BinaryPSO, will\n",
    "    be used to mask features.\n",
    "alpha: float (default is 0.5)\n",
    "    Constant weight for trading-off classifier performance\n",
    "    and number of features\n",
    "\n",
    "Returns\n",
    "-------\n",
    "numpy.ndarray\n",
    "    Computed objective function\n",
    "\"\"\"\n",
    "\n",
    "# Create an instance of the classifier\n",
    "classifier = RandomForestClassifier(max_depth=2,n_estimators=10)\n",
    "\n",
    "# Define objective function\n",
    "def f_per_particle(m, alpha):\n",
    "\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "    \n",
    "    # Get the subset of the features from the binary mask\n",
    "    if np.count_nonzero(m) == 0:\n",
    "        X_train_subset = X_train\n",
    "        X_test_subset = X_test\n",
    "    else:\n",
    "        X_train_subset = X_train[:,m==1]\n",
    "        X_test_subset = X_test[:,m==1]\n",
    "        \n",
    "    # Perform classification and store performance in P\n",
    "    classifier.fit(X_train_subset, y_train)\n",
    "    P = (classifier.predict(X_test_subset) == y_test).mean()\n",
    "\n",
    "    # Compute for the objective function\n",
    "    j = (alpha * (1.0 - P)\n",
    "        + (1.0 - alpha) * (1 - (X_train_subset.shape[1] /X_train.shape[1])))\n",
    "    \n",
    "    return j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next** we define the higher level objective function to be evaluated in pyswarms built in optimiser. \n",
    "\n",
    "It returns the object loss score for *all* particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Higher-level method to do classification in the\n",
    "whole swarm.\n",
    "\n",
    "Inputs\n",
    "------\n",
    "M: numpy.ndarray of shape (n_particles, dimensions)\n",
    "    The swarm that will perform the search\n",
    "\n",
    "Returns\n",
    "-------\n",
    "numpy.ndarray of shape (n_particles, )\n",
    "    The computed loss for each particle\n",
    "\"\"\"\n",
    "\n",
    "def f(M, alpha=0.99):\n",
    "\n",
    "    n_particles = M.shape[0]\n",
    "    \n",
    "    j = [f_per_particle(M[i], alpha) for i in range(n_particles)]\n",
    "\n",
    "    return np.array(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Set-up the optimisation parameters and run the selections algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `options` dictionary below: \n",
    "- `k`represents the neighbors to be considered when calculating the best known position of the swarm\n",
    "- `p`represents a distance metric used in the optimisation algorithm. \n",
    "\n",
    "**Question** What are the purpose of the other three parameters? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-12 16:38:55,505 - pyswarms.discrete.binary - INFO - Optimize for 20 iters with {'c1': 1.5, 'c2': 1.5, 'w': 0.5, 'k': 4, 'p': 2}\n",
      "pyswarms.discrete.binary: 100%|██████████████████████████|20/20, best_cost=0.264\n",
      "2022-05-12 16:40:05,246 - pyswarms.discrete.binary - INFO - Optimization finished | best cost: 0.26430000000000003, best pos: [1 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1 1 0 0 1 1 1 0 1 0 1 0 1\n",
      " 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1\n",
      " 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 0\n",
      " 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1\n",
      " 0 0 0 0 1 0 0 0 0 1 1 0 1 1 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimiser ran for 69.7429 seconds\n",
      "Optimiser retained 110 features\n"
     ]
    }
   ],
   "source": [
    "# Initialize swarm, arbitrary\n",
    "options = {'c1': 1.5, 'c2': 1.5, 'w':0.5, 'k': 4, 'p':2}\n",
    "\n",
    "# Call instance of PSO\n",
    "dimensions = X.shape[1] # dimensions should be the number of features\n",
    "\n",
    "optimizer = ps.discrete.BinaryPSO(n_particles=60, dimensions=dimensions, options=options)\n",
    "\n",
    "# Perform optimization\n",
    "tic = time.perf_counter()\n",
    "cost, pos = optimizer.optimize(f, iters=20, verbose=2)\n",
    "toc = time.perf_counter()\n",
    "print(f\"Optimiser ran for {toc - tic:0.4f} seconds\")\n",
    "selected = np.count_nonzero(pos)\n",
    "print(\"Optimiser retained \" + str(selected)   + \" features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evalute the performamce of the selected features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the effectiveness of the selected features, we can use two Random Forest Classifiers, one trained on the full feature set, the other trained on the selected feature sets, and compare the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Feature set performance: 0.650\n",
      "Subset Feature set performance: 0.677\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "c1 = RandomForestClassifier(max_depth=2, n_estimators=10, random_state=42)\n",
    "c1.fit(X_train, y_train)\n",
    "full_performance = (c1.predict(X_test) == y_test).mean()\n",
    "print('Full Feature set performance: %.3f' % (full_performance))\n",
    "\n",
    "c2 = RandomForestClassifier(max_depth=2, n_estimators=10, random_state=42)\n",
    "c2.fit(X_train[:,pos==1], y_train)\n",
    "subset_performance = (c2.predict(X_test[:,pos==1]) == y_test).mean()\n",
    "print('Subset Feature set performance: %.3f' % (subset_performance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how altering key parameters changes the performance and running time of the optimiser.\n",
    "\n",
    "Parameters to consider altering include\n",
    "- alpha\n",
    "- Swarm options\n",
    "- Number of particles\n",
    "- Number of optimiser iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-12 15:34:57,859 - pyswarms.discrete.binary - INFO - Optimize for 10 iters with {'c1': 1.1, 'c2': 1.1, 'w': 0.9, 'k': 2, 'p': 2}\n",
      "pyswarms.discrete.binary: 100%|██████████████████████████|10/10, best_cost=0.313\n",
      "2022-05-12 15:35:58,640 - pyswarms.discrete.binary - INFO - Optimization finished | best cost: 0.3134848484848485, best pos: [1 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 0 0 0 0 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 0 1 0\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1\n",
      " 1 0 1 1 0 1 0 0 0 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1\n",
      " 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 0 0 0 1 1 1\n",
      " 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimiser ran for 60.7822 seconds\n",
      "Optimiser retained 130 features\n",
      "Full Feature set performance: 0.650\n",
      "Subset Feature set performance: 0.665\n"
     ]
    }
   ],
   "source": [
    "# Alpha is to control weight between model performances and num of features selected (previous: 0.5)\n",
    "# c1 -> cognitive param, c2 -> social param, w -> inertia, k -> num of nearest particles, p -> distance\n",
    "alpha = 0.1\n",
    "options = {'c1': 1.1, 'c2': 1.1, 'w':0.9, 'k': 2, 'p':2}\n",
    "\n",
    "optimizer = ps.discrete.BinaryPSO(n_particles=100, dimensions=dimensions, options=options)\n",
    "\n",
    "# Perform optimization\n",
    "tic = time.perf_counter()\n",
    "cost, pos = optimizer.optimize(f, iters=10, verbose=2)\n",
    "toc = time.perf_counter()\n",
    "print(f\"Optimiser ran for {toc - tic:0.4f} seconds\")\n",
    "selected = np.count_nonzero(pos)\n",
    "print(\"Optimiser retained \" + str(selected)   + \" features\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "c1 = RandomForestClassifier(max_depth=2, n_estimators=10, random_state=42)\n",
    "c1.fit(X_train, y_train)\n",
    "full_performance = (c1.predict(X_test) == y_test).mean()\n",
    "print('Full Feature set performance: %.3f' % (full_performance))\n",
    "\n",
    "c2 = RandomForestClassifier(max_depth=2, n_estimators=10, random_state=42)\n",
    "c2.fit(X_train[:,pos==1], y_train)\n",
    "subset_performance = (c2.predict(X_test[:,pos==1]) == y_test).mean()\n",
    "print('Subset Feature set performance: %.3f' % (subset_performance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement feature selection on the *Breast Cancer Wisconsin Diagnostic Dataset*. The features present in this dataset are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. The features are labelled as coming from either a malignant or benign sample.\n",
    "\n",
    "The data can be imported directly from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry\n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        worst/largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
      "        10 is Radius SE, field 20 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer()\n",
    "print(data.DESCR) \n",
    "\n",
    "X, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.09706398, -2.07333501,  1.26993369, ...,  2.29607613,\n",
       "         2.75062224,  1.93701461],\n",
       "       [ 1.82982061, -0.35363241,  1.68595471, ...,  1.0870843 ,\n",
       "        -0.24388967,  0.28118999],\n",
       "       [ 1.57988811,  0.45618695,  1.56650313, ...,  1.95500035,\n",
       "         1.152255  ,  0.20139121],\n",
       "       ...,\n",
       "       [ 0.70228425,  2.0455738 ,  0.67267578, ...,  0.41406869,\n",
       "        -1.10454895, -0.31840916],\n",
       "       [ 1.83834103,  2.33645719,  1.98252415, ...,  2.28998549,\n",
       "         1.91908301,  2.21963528],\n",
       "       [-1.80840125,  1.22179204, -1.81438851, ..., -1.74506282,\n",
       "        -0.04813821, -0.75120669]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(M, alpha=1):\n",
    "\n",
    "    n_particles = M.shape[0]\n",
    "    \n",
    "    j = [f_per_particle(M[i], alpha) for i in range(n_particles)]\n",
    "\n",
    "    return np.array(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-12 16:31:07,431 - pyswarms.discrete.binary - INFO - Optimize for 10 iters with {'c1': 1.9, 'c2': 1.9, 'w': 0.5, 'k': 10, 'p': 2}\n",
      "pyswarms.discrete.binary: 100%|█████████████████████████|10/10, best_cost=0.0106\n",
      "2022-05-12 16:31:32,540 - pyswarms.discrete.binary - INFO - Optimization finished | best cost: 0.010638297872340385, best pos: [0 0 1 1 0 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimiser ran for 25.1106 seconds\n",
      "Optimiser retained 19 features\n",
      "Full Feature set performance: 0.931\n",
      "Subset Feature set performance: 0.941\n"
     ]
    }
   ],
   "source": [
    "options = {'c1': 1.9, 'c2': 1.9, 'w':0.5, 'k': 10, 'p':2}\n",
    "\n",
    "# Call instance of PSO\n",
    "dimensions = X.shape[1] # dimensions should be the number of features\n",
    "\n",
    "optimizer = ps.discrete.BinaryPSO(n_particles=200, dimensions=dimensions, options=options)\n",
    "\n",
    "# Perform optimization\n",
    "tic = time.perf_counter()\n",
    "cost, pos = optimizer.optimize(f, iters=10, verbose=2)\n",
    "toc = time.perf_counter()\n",
    "print(f\"Optimiser ran for {toc - tic:0.4f} seconds\")\n",
    "selected = np.count_nonzero(pos)\n",
    "print(\"Optimiser retained \" + str(selected)   + \" features\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "c1 = RandomForestClassifier(max_depth=2, n_estimators=10, random_state=42)\n",
    "c1.fit(X_train, y_train)\n",
    "full_performance = (c1.predict(X_test) == y_test).mean()\n",
    "print('Full Feature set performance: %.3f' % (full_performance))\n",
    "\n",
    "c2 = RandomForestClassifier(max_depth=2, n_estimators=10, random_state=42)\n",
    "c2.fit(X_train[:,pos==1], y_train)\n",
    "subset_performance = (c2.predict(X_test[:,pos==1]) == y_test).mean()\n",
    "print('Subset Feature set performance: %.3f' % (subset_performance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "174px",
    "left": "1897px",
    "right": "20px",
    "top": "303px",
    "width": "467px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
