{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snippets of code to get started on the AI Assignment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains some useful snippets of code to get you started with the assignment. The practical worksheets as well as the __[scikit-learn](https://scikit-learn.org/stable/)__ and __[keras](https://keras.io)__ webpages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code presumes the supplied *eGeMAPS* folder is located in the same directory as this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data in Pandas data frames and remove unecessary information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data as CSV, note the delimiter in these files is ;\n",
    "Training_data = pd.read_csv(\"eGeMAPS/train.csv\",sep=';')\n",
    "Training_data = Training_data.drop(['name', 'frameTime'], axis=1)\n",
    "\n",
    "Development_data = pd.read_csv(\"eGeMAPS/devel.csv\",sep=';')\n",
    "Development_data = Development_data.drop(['name', 'frameTime'], axis=1)\n",
    "\n",
    "Test_data = pd.read_csv(\"eGeMAPS/test.csv\",sep=';')\n",
    "Test_data = Test_data.drop(['name', 'frameTime'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the pandas dataframes to Numpy arrarys named `X_Train`, `X_Val` and `X_Test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = Training_data.values\n",
    "X_devel = Development_data.values\n",
    "X_test = Test_data.values\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_devel.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to read in labels and converts them to *single column* numpy arrary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code presumes the supplied *Labels* folder is located in the same directory as this notebook\n",
    "\n",
    "**Reminder** Labels are *not* provided for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data as CSV, note the delimiter in these files is ;\n",
    "Training_labels = pd.read_csv(\"Labels/train.csv\")\n",
    "Training_labels = Training_labels.drop(['file_name'], axis=1)\n",
    "\n",
    "Development_labels = pd.read_csv(\"Labels/devel.csv\")\n",
    "Development_labels = Development_labels.drop(['file_name'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to produce a single column numpy arrays named `Y_Train`, `Y_Val`\n",
    " - **Note:** 0 denotes \"cold\" and 1 denotes \"not cold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(['C', 'NC'])\n",
    "y_train = le.transform(np.ravel(Training_labels))\n",
    "y_devel = le.transform(np.ravel(Development_labels))\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_devel.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example code for training a SVM with training  data and testing on development data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code uses a min-max scaler to scale both the training and development data so all the values of all features is between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_train_scaled = min_max_scaler.fit_transform(X_train)\n",
    "X_devel_scaled = min_max_scaler.transform(X_devel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a linear SVM using the scaled training data. \n",
    "- **Note** There are no guarantees this is a suitable classifier set-up for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svmClas_TR_1 = SVC(kernel='linear', C = 0.01, random_state=42) \n",
    "svmClas_TR_1.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block demostrates how to perform prediction on the (scaled) developement data and then score the performance using accuracy and plots the resulting confusion matrix\n",
    "- **Note** There are no guarantees this is a suitable performance metric for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = svmClas_TR_1.predict(X_devel_scaled)\n",
    "\n",
    "acc = accuracy_score(y_devel, y_pred)\n",
    "\n",
    "print( \"Classification of SVM: Accuracy \" + str(round(acc,3)))\n",
    "\n",
    "cf_matrix = confusion_matrix(y_devel, y_pred)\n",
    "sns.heatmap(cf_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example code for training a feedward (dense) neural network with the training data and testing on development data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import the required functions\n",
    "- **Note** you may need to change these to suit your TensorFlow/Keras set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our standard printing function to display loss and accuracy over the training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learningCurve(history, epoch):\n",
    "  # Plot training & validation accuracy values\n",
    "  epoch_range = range(1, epoch+1)\n",
    "  plt.plot(epoch_range, history.history['accuracy'])\n",
    "  plt.plot(epoch_range, history.history['val_accuracy'])\n",
    "  plt.title('Model accuracy')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Val'], loc='upper left')\n",
    "  plt.show()\n",
    "\n",
    "  # Plot training & validation loss values\n",
    "  plt.plot(epoch_range, history.history['loss'])\n",
    "  plt.plot(epoch_range, history.history['val_loss'])\n",
    "  plt.title('Model loss')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Val'], loc='upper left')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to convert the training labels to one-hot vecotrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_vec = to_categorical(y_train, 2)\n",
    "y_devel_vec = to_categorical(y_devel, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to normalise the data so that the distribution of each feature of the data partitions has a mean of zeor and unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_devel_scaled = scaler.transform(X_devel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare a new sequential keras model\n",
    "- **Note** Again, there are no guarantees this is a suitable classifier set-up for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "def create_FF_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=n_features, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to train the network on the scaled training data\n",
    "- **Note** Again, there are no guarantees the parameters given below are suitable for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X_train_scaled.shape[1]\n",
    "\n",
    "model_FF_TR_1 = create_FF_model()\n",
    "\n",
    "model_FF_TR_1.compile(optimizer='adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "EPOCHS = 64\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "history = model_FF_TR_1.fit(\n",
    "    X_train_scaled, y_train_vec, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    epochs=EPOCHS,\n",
    "    validation_split = 0.2,\n",
    "    shuffle=True,\n",
    "    verbose=0\n",
    ")\n",
    "plot_learningCurve(history, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the performance of the model on the scaled development set data\n",
    "- **Note** Again, there are no guarantees this is a suitable performance metric for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_vec = model_FF_TR_1.predict(X_devel_scaled)\n",
    "\n",
    "# This line converts the softmax output back to a single column array\n",
    "y_pred = np.argmax(y_pred_vec, axis=1)\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_devel, y_pred)\n",
    "\n",
    "print( \"Classification of Network: Accuracy \" + str(round(acc,3)))\n",
    "\n",
    "cf_matrix = confusion_matrix(y_devel, y_pred)\n",
    "sns.heatmap(cf_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine training and developement data for final testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the feature sets\n",
    "X_tr_dv = np.concatenate((X_train, X_devel), axis=0)\n",
    "\n",
    "#Combine the labels\n",
    "y_tr_dv = np.concatenate((y_train, y_devel), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example code for training a SVM with training and development data and generating the test samples for external validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reminder** There are no guarantees this is a suitable classifier set-up for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the new combined feature space and test data\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_tr_dv_scaled = min_max_scaler.fit_transform(X_tr_dv)\n",
    "X_test_scaled = min_max_scaler.transform(X_test)\n",
    "\n",
    "#Train the classifier on the new combined scaled feature space\n",
    "svmClas_CFV = SVC(kernel='linear', C = 0.01, random_state=42) \n",
    "svmClas_CFV.fit(X_tr_dv_scaled, y_tr_dv)\n",
    "\n",
    "#Obtain test set prediction using the scaled data\n",
    "y_test_pred = svmClas_CFV.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to convert test prediction into a CSV file. The produced CSV will be in the same folder as this notebook\n",
    "- Remember to adjust name and trial number to suit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(['C', 'NC'])\n",
    "Test_predictions = le.inverse_transform(np.ravel(y_test_pred))\n",
    "Test_predictions = pd.DataFrame(Test_predictions)\n",
    "\n",
    "Test_predictions.to_csv('Cummins_Trial_1.csv', index=False, na_rep='NaN') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example code for training a feedward (dense) neural network with training and development data and generating the test samples for external validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reminder** There are no guarantees this is a suitable classifier set-up for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the new combined feature space and test data\n",
    "scaler = StandardScaler()\n",
    "X_tr_dv_scaled = scaler.fit_transform(X_tr_dv)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert the combined lables to one-hot vectors for softmax\n",
    "y_tr_dv_vec = to_categorical(y_tr_dv, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X_tr_dv_scaled.shape[1]\n",
    "\n",
    "#As we are using the same model architecture, I have not created a new function call\n",
    "model_FF_TRDV_1 = create_FF_model()\n",
    "\n",
    "model_FF_TRDV_1.compile(optimizer='adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "EPOCHS = 64\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "history = model_FF_TRDV_1.fit(\n",
    "    X_tr_dv_scaled, y_tr_dv_vec, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    epochs=EPOCHS,\n",
    "    validation_split = 0.2,\n",
    "    shuffle=True,\n",
    "    verbose=0\n",
    ")\n",
    "plot_learningCurve(history, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain test set prediction using the scaled data and convert them back into a single colum vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_vec = model_FF_TRDV_1.predict(X_test_scaled)\n",
    "y_test_pred = np.argmax(y_test_pred_vec, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to convert test prediction into a CSV file. The produced CSV will be in the same folder as this notebook\n",
    "- Remember to adjust name and trial number to suit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(['C', 'NC'])\n",
    "Test_predictions = le.inverse_transform(np.ravel(y_test_pred))\n",
    "Test_predictions = pd.DataFrame(Test_predictions)\n",
    "\n",
    "Test_predictions.to_csv('Cummins_Trial_2.csv', index=False, na_rep='NaN') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "233px",
    "left": "1575px",
    "right": "20px",
    "top": "365px",
    "width": "632px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
